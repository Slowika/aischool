{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing\n",
    "\n",
    "Installing [PyTorch](http://pytorch.org) is fairly easy:  \n",
    "`pip install torch torchvision` or `conda install torch torchvision`\n",
    "\n",
    "Note:  \n",
    "- This tutorial is written for Python3.6+, which I strongly recommend using. It has lots of wholesome features!\n",
    "- This file is a [Jupyter Notebook](https://jupyter.org/) `pip install jupyter; jupyter notebook`, which is a very handy way of presenting python experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch?\n",
    "\n",
    "PyTorch...\n",
    "- is a numerical computation library,\n",
    "- has both CPU and GPU backends,\n",
    "- has a seamless automatic differentiation package, think gradients, $\\nabla_\\theta f$\n",
    "- is finetuned for Deep Learning, but can be used for anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental data structure in PyTorch is the `Tensor`, which is a multidimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = tensor([5, 7, 9])\n",
      "a - b = tensor([-3, -3, -3])\n",
      "a * b = tensor([ 4, 10, 18])\n",
      "a^T b = tensor(32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "print('a + b =', a + b)   # These 3 operations are `elementwise`\n",
    "print('a - b =', a - b)\n",
    "print('a * b =', a * b)\n",
    "print('a^T b =', a.dot(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have a shape, and thus a certain number of dimensions, that can both be dynamically changed. Keep in mind when reshaping that the array data is row-major."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([[1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 1.]])\n",
      "\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.eye(4, 4)\n",
    "print(c)\n",
    "print(c.reshape((2, 8)))\n",
    "d = torch.arange(9)\n",
    "print()\n",
    "print(d)\n",
    "print(d.reshape((3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays can be indexed and sliced in various ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing: \n",
      "tensor(4)\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 1],\n",
      "        [3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[4, 5],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "d = d.reshape((3, 3))\n",
    "print('indexing: ')\n",
    "print(d[1, 1])\n",
    "print(d[:2])\n",
    "print(d[:, :2])\n",
    "print(d[1:3, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "PyTorch's tensor elementwise operations follow *broadcasting* rules when the shapes differ:\n",
    "> Two tensors are “broadcastable” if the following rules hold:\n",
    "> - Each tensor has at least one dimension.\n",
    "> - When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "\n",
    "What does this mean?\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a and b's shapes: torch.Size([3]) torch.Size([1])\n",
      "a + b: tensor([11, 12, 13])\n",
      "a + b shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([10])\n",
    "print('a and b\\'s shapes:', a.shape, b.shape)\n",
    "# Since b.shape[0] == 1, `10` is \"broadcast\", i.e. virtually copied, across a's 0th dimension\n",
    "# So [1, 2, 3] + [10] is equivalent to [1, 2, 3] + [10, 10, 10]\n",
    "print('a + b:', a + b)\n",
    "print('a + b shape:',(a + b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3, 2])\n",
      "That did not work, as expected.\n",
      "tensor([[3.1416, 0.0000, 0.0000],\n",
      "        [0.0000, 3.1416, 0.0000],\n",
      "        [0.0000, 0.0000, 3.1416]])\n"
     ]
    }
   ],
   "source": [
    "# More complicated:\n",
    "# - since the second tensor only has 3 dimensions, a dimension of length `1` is implicitly added.\n",
    "# - the 0th and 2nd dimensions are now of length `1` in the second tensor, so they are broadcasted!\n",
    "print((torch.ones(5,4,3,2) + \n",
    "       torch.ones(  4,1,2)).shape)\n",
    "\n",
    "# This does not work! `1`-dimensions can only be added to the left \n",
    "try:\n",
    "    print((torch.ones(5,4,3,2) + \n",
    "           torch.ones(5,4,3  )).shape)\n",
    "except:\n",
    "    print(\"That did not work, as expected.\")\n",
    "    \n",
    "# scalars are implicitly broadcast to every dimension:\n",
    "print(torch.eye(3,3) * np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17],\n",
      "        [18, 19, 20]])\n",
      "tensor([[ 3,  4,  5],\n",
      "        [ 9, 10, 11],\n",
      "        [15, 16, 17]])\n",
      "tensor([ 3, 10, 15])\n",
      "tensor([[ 0,  2],\n",
      "        [ 3,  5],\n",
      "        [ 6,  8],\n",
      "        [ 9, 11],\n",
      "        [12, 14],\n",
      "        [15, 17],\n",
      "        [18, 20]])\n"
     ]
    }
   ],
   "source": [
    "# It is possible to index a dimension along specified indices\n",
    "array = torch.arange(7*3).reshape((7,3))\n",
    "print(array)\n",
    "# This is equivalent to [array[1], array[3], array[5]]\n",
    "print(array[[1, 3, 5]])\n",
    "# Indexing over multiple dimensions is also allowed:\n",
    "# This is equivalent to [array[1, 0], array[3, 1], array[5, 0]]\n",
    "print(array[[1, 3, 5], [0, 1, 0]])\n",
    "\n",
    "print(array[:, [0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d[None, :2, None, 1:].shape =  torch.Size([1, 2, 1, 2])\n",
      "\n",
      "Does A == B == C? True\n",
      "tensor([[ 15,  18,  21],\n",
      "        [ 42,  54,  66],\n",
      "        [ 69,  90, 111]])\n"
     ]
    }
   ],
   "source": [
    "# `None` can be used to add dimesions\n",
    "print('d[None, :2, None, 1:].shape = ', d[None, :2, None, 1:].shape) \n",
    "\n",
    "# These dimensions become of length 1, which is broadcastable!\n",
    "# For example a matrix multiply is:\n",
    "A = (d[:, None, :] * d.transpose(0, 1)[:, :, None]).sum(dim=0)\n",
    "# but this is more efficient:\n",
    "B = torch.mm(d, d)\n",
    "# and this is more elegant, if you're into that:\n",
    "C = torch.einsum('ij,jk->ik', d, d)\n",
    "\n",
    "print('\\nDoes A == B == C?',\n",
    "      bool((A == B).all() and (B == C).all())) # Checking that `all` elements are equal\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parenthesis on einsum\n",
    "Parenthesis on **einsum**, tensor product-sums in Einstein summation notation:\n",
    "\n",
    "`C = einsum('ijk,kji->j', A, B)` is equivalent to (`A` and `B` are 3D Tensors)\n",
    "$$C_j = \\sum_i\\sum_k A_{ijk} B_{kji}$$\n",
    "\n",
    "`C = einsum('ij,k->ijk', A, B)` is equivalent to (`A` a matrix, `B` a vector)\n",
    "$$C_{ijk} = A_{ij} B_k$$\n",
    "`C = einsum('ij,k->ij', A, B)` is equivalent to (idem)\n",
    "$$C_{ij} = \\sum_k A_{ij} B_k$$\n",
    "\n",
    "Whenever an index is used for two or more tensors (e.g. `i` in the first example), the shape **has** to match along the given dimensions. \n",
    "\n",
    "Even if the dimension shapes match, you don't have to use the same letter, e.g.:\n",
    "\n",
    "`C = einsum('ij,kl->', A, B)` is equivalent to (`A` and `B` matrices)\n",
    "$$C = \\sum_i \\sum_j\\sum_k\\sum_l A_{ij}B_{kl}$$\n",
    "note the lack of index after `->`, which makes $C\\in\\mathbb{R}$ a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "This is the main feature of PyTorch! It makes gradient-based optimization extremely easy to implement.\n",
    "\n",
    "All tensors have the potential to store gradient information, but they must be set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 0.4, 3, 0.6]], dtype=torch.float, requires_grad=True)\n",
    "W = torch.tensor(np.random.uniform(-1, 1, (4, 4)), dtype=torch.float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now whenever we compute things with `x` and `W`, PyTorch keeps track of the operations in a computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5122, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = torch.tanh(x.mm(W))\n",
    "u = torch.sum(h ** 2)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the we have a scalar \"loss\" `u`, calling `u.backward()` will fill `x` and `W`'s `grad` attribute with $\\nabla_xu$ and $\\nabla_Wu$ respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1653, -0.4894,  0.3756,  0.5888]])\n",
      "tensor([[ 0.0306, -0.2640, -0.3043, -0.2985],\n",
      "        [ 0.0122, -0.1056, -0.1217, -0.1194],\n",
      "        [ 0.0918, -0.7920, -0.9128, -0.8955],\n",
      "        [ 0.0184, -0.1584, -0.1826, -0.1791]])\n"
     ]
    }
   ],
   "source": [
    "u.backward()\n",
    "print(x.grad)\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful, this is additive! Calling backward again with a different loss will *add* to `grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2250, -0.7066,  0.5252,  0.8447]])\n",
      "tensor([[ 0.0259, -0.3772, -0.4358, -0.4274],\n",
      "        [ 0.0104, -0.1509, -0.1743, -0.1710],\n",
      "        [ 0.0777, -1.1317, -1.3074, -1.2821],\n",
      "        [ 0.0155, -0.2263, -0.2615, -0.2564]])\n"
     ]
    }
   ],
   "source": [
    "h = torch.tanh(x.mm(W))\n",
    "v = torch.sum(torch.exp(-h / 2))\n",
    "v.backward()\n",
    "print(x.grad)\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But gradients can be zeroed out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0596, -0.2172,  0.1496,  0.2559]])\n",
      "tensor([[-0.0047, -0.1132, -0.1315, -0.1289],\n",
      "        [-0.0019, -0.0453, -0.0526, -0.0516],\n",
      "        [-0.0141, -0.3397, -0.3946, -0.3866],\n",
      "        [-0.0028, -0.0679, -0.0789, -0.0773]])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_() # Side note, all torch functions ending in \"_\" mean that they change something in-place.\n",
    "W.grad.zero_()\n",
    "\n",
    "# Let's do it again and check that it is different\n",
    "h = torch.tanh(x.mm(W))\n",
    "v = torch.sum(torch.exp(-h / 2))\n",
    "v.backward()\n",
    "print(x.grad)\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Now that we can compute gradients, we can try to run optimization methods.\n",
    "\n",
    "Let's start with just gradient descent, for that we will need some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n",
      "[[5.1 3.5 1.4 0.2 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from io import StringIO\n",
    "# Fetch the data\n",
    "iris = urllib.request.urlopen(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\").read()\n",
    "# Replace string labels by class numbers\n",
    "iris = (iris\n",
    "        .decode('ascii')\n",
    "        .replace('Iris-setosa', '0')\n",
    "        .replace('Iris-versicolor', '1')\n",
    "        .replace('Iris-virginica', '2'))\n",
    "# Parse into a numpy array\n",
    "iris = np.loadtxt(StringIO(iris), delimiter=',')\n",
    "print(iris.shape)\n",
    "print(iris[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)  # Just to keep things constant from run to run\n",
    "torch.random.manual_seed(1)\n",
    "# Let's make a training set and a test set:\n",
    "indices = np.arange(len(iris))\n",
    "np.random.shuffle(indices)\n",
    "train_split, test_split = indices[:100], indices[100:]\n",
    "\n",
    "# The last column are the Ys\n",
    "trainX, trainY = iris[train_split][:, :-1], iris[train_split][:, -1]\n",
    "testX, testY = iris[test_split][:, :-1], iris[test_split][:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.6988282203674316\n",
      "200 0.5784186720848083\n",
      "300 0.5177507400512695\n",
      "400 0.4786124527454376\n",
      "500 0.4496915340423584\n",
      "600 0.4265597462654114\n",
      "700 0.4071331024169922\n",
      "800 0.3902965188026428\n",
      "900 0.37539300322532654\n",
      "1000 0.3620048761367798\n",
      "\n",
      "Our training error is  4.00%\n",
      "Our test error is      6.00%\n"
     ]
    }
   ],
   "source": [
    "W = torch.nn.Parameter(torch.zeros((4, 3)).float()) # There are 4 features and 3 classes\n",
    "W.data.uniform_(-0.01, 0.01) # Fill with random weights\n",
    "\n",
    "b = torch.nn.Parameter(torch.zeros((3,)).float())\n",
    "\n",
    "# The optimizer class takes in parameters and a learning rate,\n",
    "# and sometimes, more hyper-parameters such as momentum rate.\n",
    "optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
    "# Try playing with the learning rate to get a lower test error!\n",
    "\n",
    "\n",
    "x = torch.tensor(trainX).float()\n",
    "y = torch.tensor(trainY).long() # long because they are class labels\n",
    "for epoch in range(1001):\n",
    "    optimizer.zero_grad() # Reset the gradients\n",
    "    yhat = torch.mm(x, W) + b\n",
    "    loss = torch.nn.functional.cross_entropy(yhat, y)\n",
    "    loss.backward()  # Compute the gradients\n",
    "    optimizer.step() # Update the parameters\n",
    "    if epoch and not epoch % 100:\n",
    "        print(epoch, loss.item())\n",
    "        \n",
    "print()\n",
    "train_class_predictions = yhat.argmax(1)\n",
    "train_error_rate = (train_class_predictions != y).float().mean()\n",
    "print(f'Our training error is {100*train_error_rate: 2.2f}%')\n",
    "x = torch.tensor(testX).float()\n",
    "y = torch.tensor(testY).long()\n",
    "yhat = torch.mm(x, W) + b\n",
    "test_class_predictions = yhat.argmax(1)\n",
    "test_error_rate = (test_class_predictions != y).float().mean()\n",
    "print(f'Our test error is     {100*test_error_rate: 2.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "Let's see if neural networks can perform better on this simple task. I only managed to get as low as 4% test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.8079302310943604\n",
      "1000 0.5909492373466492\n",
      "1500 0.410470575094223\n",
      "2000 0.2573563754558563\n",
      "2500 0.14693450927734375\n",
      "3000 0.09226757287979126\n",
      "3500 0.07226601988077164\n",
      "4000 0.06423167884349823\n",
      "4500 0.05952752009034157\n",
      "5000 0.05623064935207367\n",
      "\n",
      "Our training error is  1.00%\n",
      "Our test error is      4.00%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(1)  # Just to keep things constant from run to run\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3))\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.99), weight_decay=1e-2)\n",
    "\n",
    "x = torch.tensor(trainX).float()\n",
    "y = torch.tensor(trainY).long() # long because they are class labels\n",
    "for epoch in range(5001):\n",
    "    opt.zero_grad() # Reset the gradients\n",
    "    # equivalent to\n",
    "    # for w in model.parameters():\n",
    "    #     w.grad.data.zero_()\n",
    "    yhat = model(x)\n",
    "    loss = torch.nn.functional.cross_entropy(yhat, y)\n",
    "    loss.backward()  # Compute the gradients\n",
    "    opt.step() # Update the parameters\n",
    "    if epoch and not epoch % 500:\n",
    "        print(epoch, loss.item())\n",
    "        \n",
    "print()\n",
    "train_class_predictions = yhat.argmax(1)\n",
    "train_error_rate = (train_class_predictions != y).float().mean()\n",
    "print(f'Our training error is {100*train_error_rate: 2.2f}%')\n",
    "x = torch.tensor(testX).float()\n",
    "y = torch.tensor(testY).long()\n",
    "yhat = model(x)\n",
    "test_class_predictions = yhat.argmax(1)\n",
    "test_error_rate = (test_class_predictions != y).float().mean()\n",
    "print(f'Our test error is     {100*test_error_rate: 2.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to sample from a random vector and send to GPU\n",
    "X = np.random.uniform(0, 1,(1000, 4)) # CPU\n",
    "minibatch_size = 32\n",
    "\n",
    "device = torch.device('cuda')\n",
    "# This is required to send the parameters of `model` to the GPU\n",
    "model.to(device)\n",
    "for i in range(100):\n",
    "    indices = np.random.randint(0, X.shape[0], minibatch_size)\n",
    "    minibatch_x = torch.tensor(X[indices]).float().to(device) # GPU\n",
    "    prediction = model(minibatch_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can specify which GPU to use before running your program by launching python/jupyter as follows:  \n",
    "`CUDA_VISIBLE_DEVICES=2 python code.py`  \n",
    "or  \n",
    "`CUDA_VISIBLE_DEVICES=2 jupyter notebook`\n",
    "\n",
    "Note that GPUs are 0-indexed, so e.g. go 0-3 if you have 4 GPUs on your machine.\n",
    "\n",
    "For multigpu and more check [the docs](https://pytorch.org/docs/stable/notes/cuda.html).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
